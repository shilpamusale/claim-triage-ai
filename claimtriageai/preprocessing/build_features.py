"""
Module: build_features.py

Description:
This script performs preprocessing and feature
engineering on raw healthcare claims data
to produce a clean dataset for downstream
denial prediction modeling.

Features:
- Loads data/raw_claims.csv
- Applies text cleaning to denial_reason and notes
- Supports legacy and EDI 837 schemas
- Computes structured features (claim age, denial history, patient age, etc.)
- Applies transformers (encoding, imputation)
- Saves data/processed_claims.csv
- Can be run as a script or imported as a module

Author: ClaimTriageAI Team
"""

from pathlib import Path
from typing import Any, cast

import joblib
import pandas as pd
import yaml
from category_encoders import TargetEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import FunctionTransformer, StandardScaler

from claimtriageai.configs.paths import FEATURE_CONFIG_PATH
from claimtriageai.preprocessing.feature_engineering import (
    engineer_edi_features,
    engineer_features,
)
from claimtriageai.preprocessing.text_cleaning import clean_text_fields
from claimtriageai.utils.functions import convert_to_int
from claimtriageai.utils.logger import get_logger

# Initialize Logging
logger = get_logger("preprocessing")


# ------------------------- Core Logic -------------------------


def load_feature_config() -> dict[str, Any]:
    with open(FEATURE_CONFIG_PATH, "r") as f:
        config = yaml.safe_load(f)
    return cast(dict[str, Any], config)


def preprocess_and_save(raw_path: str, output_path: str, transformer_dir: str) -> None:
    logger.info(f"Loading raw data from {raw_path}")
    df = pd.read_csv(raw_path)

    # Validate required label column
    assert (
        "denied" in df.columns
    ), "'denied' column must be present in raw data for supervised learning."

    logger.info("Cleaning text fields...")
    df = clean_text_fields(df)

    # Load feature config
    config = load_feature_config()

    # EDI fields
    edi_fields = config.get(
        "edi_fields",
        [
            "patient_dob",
            "service_date",
            "patient_gender",
            "facility_code",
            "billing_provider_specialty",
            "claim_type",
            "prior_authorization",
            "accident_indicator",
        ],
    )
    edi_coverage = len([col for col in edi_fields if col in df.columns]) / len(
        edi_fields
    )
    if edi_coverage >= 0.5:
        logger.info(
            f"Detected EDI schema ({edi_coverage:.0%} coverage) â€” applying EDI features"
        )
        df = engineer_edi_features(df)

    # Extract label and apply full feature engineering
    y = df["denied"]
    logger.info("Engineering structured features...")
    df = engineer_features(df, y)

    logger.info("Splitting features and target...")
    X = df.drop(columns=["denied"])

    # Categorical columns
    categorical_cols = config.get(
        "categorical_features_target_encoded",
        [
            "payer_id",
            "provider_type",
            "plan_type",
            "claim_type",
            "billing_provider_specialty",
            "facility_code",
            "diagnosis_code",
            "procedure_code",
        ],
    )
    categorical_cols = [col for col in categorical_cols if col in X.columns]

    logger.info("Applying TargetEncoder to categorical features...")
    target_encoder = TargetEncoder()
    X[categorical_cols] = target_encoder.fit_transform(X[categorical_cols], y)

    # Numeric features
    numeric_features = config.get(
        "numeric_features",
        [
            "claim_age_days",
            "patient_age",
            "total_charge_amount",
            "days_to_submission",
            "payer_deny_rate",
            "provider_deny_rate",
            "resubmission_rate_by_payer",
            "followup_intensity_score",
        ],
    )
    numeric_features = [f for f in numeric_features if f in X.columns]

    # Boolean features
    boolean_features = config.get(
        "boolean_features",
        [
            "is_resubmission",
            "contains_auth_term",
            "prior_authorization",
            "accident_indicator",
            "high_charge_flag",
        ],
    )
    boolean_features = [f for f in boolean_features if f in X.columns]

    logger.info("Building preprocessing pipeline...")
    preprocessor = ColumnTransformer(
        transformers=[
            (
                "num",
                Pipeline(
                    [
                        ("imputer", SimpleImputer(strategy="mean")),
                        ("scaler", StandardScaler()),
                    ]
                ),
                numeric_features,
            ),
            (
                "bool",
                Pipeline(
                    [
                        ("imputer", SimpleImputer(strategy="most_frequent")),
                        (
                            "to_int",
                            FunctionTransformer(
                                convert_to_int,
                                validate=False,
                                feature_names_out="one-to-one",
                            ),
                        ),
                    ]
                ),
                boolean_features,
            ),
        ],
        remainder="drop",
        verbose_feature_names_out=True,
    )

    logger.info("Fitting numeric+boolean transformer...")
    transformed_array = preprocessor.fit_transform(X)
    transformed_df = pd.DataFrame(
        transformed_array, columns=preprocessor.get_feature_names_out()
    )

    logger.info("Reconstructing final training matrix...")
    encoded_cat_df = X[categorical_cols].reset_index(drop=True)
    final_df = pd.concat([encoded_cat_df, transformed_df], axis=1)
    final_df["denied"] = y.values

    logger.info(f"Saving processed features to: {output_path}")
    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
    final_df.to_csv(output_path, index=False)

    logger.info("Saving transformers...")
    Path(transformer_dir).mkdir(parents=True, exist_ok=True)

    # dump(preprocessor, f"{transformer_dir}/numeric_transformer.joblib")
    # dump(target_encoder, f"{transformer_dir}/target_encoder.joblib")

    # Save the numeric transformer/preprocessor
    numeric_transformer_path = f"{transformer_dir}/numeric_transformer.joblib"
    logger.info(f"Saving numeric transformer to {numeric_transformer_path}...")
    with open(numeric_transformer_path, "wb") as f:
        joblib.dump(preprocessor, f)

    # Save the target encoder
    target_encoder_path = f"{transformer_dir}/target_encoder.joblib"
    logger.info(f"Saving target encoder to {target_encoder_path}...")
    with open(target_encoder_path, "wb") as f:
        joblib.dump(target_encoder, f)

    logger.info("Preprocessing complete.")


# ------------------------- CLI -------------------------
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--input", default="data/raw_claims.csv")
    parser.add_argument("--output", default="data/processed_claims.csv")
    parser.add_argument("--transformer_dir", default="models")

    args = parser.parse_args()
    preprocess_and_save(args.input, args.output, args.transformer_dir)
